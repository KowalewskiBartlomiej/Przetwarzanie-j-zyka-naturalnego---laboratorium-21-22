{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCK3GfLzVvcf"
      },
      "source": [
        "## Wczytywanie danych\n",
        "Paczka Sklearn (Scikit Learn) to kolejne bardzo popularne narzędzie do uczenia maszynowego. Posiada bardzo przejrzyste API i spore wsparcie (scikit-learn.org/). W dzisiejszych zadaniach skupimy się na ładowaniu zbiorów danych, ich transformacji oraz algorytmach klasyfikacji. Podczas dzisiejszych laboratoriów wykorzystamy:\n",
        "<ul>\n",
        "    <li>NLTK - udostępniające metody prostego przetwarzania tekstu (tokenizacja, lematyzacja, stemming)</li>\n",
        "    <li>Sklearn - paczkę do uczenia maszynowego</li>\n",
        "    <li>Pandas - bibliotekę do wczytywania i obsługi zbiorów danych</li>\n",
        "</ul>\n",
        "<span style=\"color: #ff0000\">Ponieważ część kodu jest już stworzona, w każdym zadaniu wyszczególnione są numery linii, w których należy wprowadzić modyfikacje, aby rozwiązać zadanie. Jeśli nie widzisz numeracji linii w kodzie w otwartym notebooku - możesz włączyć tę funkcjonalność poprzez wybór View -> toggle line numbers w górnym menu.</span><br/><br/>\n",
        "Najpierw wczytajmy dane tekstowe ze zbioru, w którym posiadamy zestaw wiadomości e-mail oznaczonych jako spamowe lub niespamowe.\n",
        "Ponieważ będziemy rozwiązywać problem klasyfikacji, oddzielamy dane do trenowania klasyfikatora oraz do weryfikacji jego jakości.\n",
        "<br/>\n",
        "\n",
        "<strong>Przeanalizuj i uruchom poniższy fragment kodu.</strong> Załaduje on odpowiednie dane do dwóch obiektów:\n",
        "<ol>\n",
        "<li>train: zbiór treningowy - dokumenty na których nauczymy klasyfikator</li>\n",
        "<li>test: zbiór testowy - dokumenty na których przetestujemy klasyfikator</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install s3fs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HPGmsHyjyWZ",
        "outputId": "ddb46f17-4fca-4619-c68d-f98413487f68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting s3fs\n",
            "  Downloading s3fs-2022.2.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiobotocore~=2.1.0\n",
            "  Downloading aiobotocore-2.1.2.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting fsspec==2022.02.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 28.1 MB/s \n",
            "\u001b[?25hCollecting aiohttp<=4\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting botocore<1.23.25,>=1.23.24\n",
            "  Downloading botocore-1.23.24-py3-none-any.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.7/dist-packages (from aiobotocore~=2.1.0->s3fs) (1.13.3)\n",
            "Collecting aioitertools>=0.5.1\n",
            "  Downloading aioitertools-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs) (3.10.0.2)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 81.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<=4->s3fs) (2.0.12)\n",
            "Collecting typing-extensions>=3.7.4\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 63.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.25,>=1.23.24->aiobotocore~=2.1.0->s3fs) (1.15.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<=4->s3fs) (2.10)\n",
            "Building wheels for collected packages: aiobotocore\n",
            "  Building wheel for aiobotocore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aiobotocore: filename=aiobotocore-2.1.2-py3-none-any.whl size=55992 sha256=bbe259ac726750673f0c7d4afd2fb64fe5d763409b81d91fae19009eee7fae1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/9e/81/732cf36b7a7e73f82ef7793b779210f0bf94e12c13b3f2a18e\n",
            "Successfully built aiobotocore\n",
            "Installing collected packages: typing-extensions, multidict, frozenlist, yarl, urllib3, jmespath, asynctest, async-timeout, aiosignal, botocore, aioitertools, aiohttp, fsspec, aiobotocore, s3fs\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
            "Successfully installed aiobotocore-2.1.2 aiohttp-3.8.1 aioitertools-0.10.0 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 botocore-1.23.24 frozenlist-1.3.0 fsspec-2022.2.0 jmespath-0.10.0 multidict-6.0.2 s3fs-2022.2.0 typing-extensions-4.1.1 urllib3-1.26.8 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "XCZc6aDCVvck",
        "outputId": "3fc5a74f-dd9a-48fd-86d0-56b88b9f28c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elementów w zbiorze treningowym: 1624, testowym: 733\n",
            "\n",
            "\n",
            "Liczność klas w zbiorze treningowym: \n",
            "ham     1111\n",
            "spam     513\n",
            "Name: label, dtype: int64\n",
            "\n",
            "\n",
            "Liczność klas w zbiorze testowym: \n",
            "ham     517\n",
            "spam    216\n",
            "Name: label, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  label                                               text  label_num\n",
              "0   ham  Re: What to choose for Core i5 64 bits?>>> If ...          0\n",
              "1  spam  Strictly Private.Gooday, With warm heart my fr...          1\n",
              "2   ham  Re: Flash is open?On Sat, 15 May 2010 00:27:32...          0\n",
              "3   ham  Re: Alsa/Redhat 8 compatabilityMatthias Saou (...          0\n",
              "4  spam  Hey hibody, Save 80% today Lixi Eights followi...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60d1bff3-28b4-4ce8-a4c7-f0d8b62ccbef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Re: What to choose for Core i5 64 bits?&gt;&gt;&gt; If ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>spam</td>\n",
              "      <td>Strictly Private.Gooday, With warm heart my fr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ham</td>\n",
              "      <td>Re: Flash is open?On Sat, 15 May 2010 00:27:32...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>Re: Alsa/Redhat 8 compatabilityMatthias Saou (...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>spam</td>\n",
              "      <td>Hey hibody, Save 80% today Lixi Eights followi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60d1bff3-28b4-4ce8-a4c7-f0d8b62ccbef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60d1bff3-28b4-4ce8-a4c7-f0d8b62ccbef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60d1bff3-28b4-4ce8-a4c7-f0d8b62ccbef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "\n",
        "# ---------------- Ładowanie danych i oddzielanie zbioru treningowego od testowego ------\n",
        "\n",
        "try:\n",
        "    full_dataset = pandas.read_csv('spam_emails.csv', encoding='utf-8')      # wczytaj dane z pliku CSV\n",
        "except:\n",
        "    import s3fs\n",
        "    full_dataset = pandas.read_csv(\"https://dwisniewski-put-pjn.s3.eu-north-1.amazonaws.com/spam_emails.csv\")\n",
        "full_dataset['label_num'] = full_dataset.label.map({'ham':0, 'spam':1})  # ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\", wykonujemy mapowanie tych wartości na liczby, co będzie potrzebne do wykonania klasyfikacji. \n",
        "\n",
        "np.random.seed(0)                                       # ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
        "train_indices = np.random.rand(len(full_dataset)) < 0.7 # wylosuj 70% danych, które stworzą zbiór treningowy. train_indices, to wektor o długości liczności wczytanego zbioru danych, w którym każda pozycja (przykład) może przyjąć dwie wartości: 1.0 - wybierz do zbioru treningowego; 0.0 - wybierz do zbioru testowego\n",
        "\n",
        "train = full_dataset[train_indices] # wybierz zbior treningowy (70%)\n",
        "test = full_dataset[~train_indices] # wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- Wyświetlanie statystyk -----------------\n",
        "\n",
        "\n",
        "print(\"Elementów w zbiorze treningowym: {train}, testowym: {test}\".format(\n",
        "    train=len(train), test=len(test)\n",
        "))\n",
        "\n",
        "print(\"\\n\\nLiczność klas w zbiorze treningowym: \")\n",
        "print(train.label.value_counts())  # wyświetl rozkład etykiet w kolumnie \"label\"\n",
        "\n",
        "print(\"\\n\\nLiczność klas w zbiorze testowym: \")\n",
        "print(test.label.value_counts())   # wyświetl rozkład etykiet w kolumnie \"label\"\n",
        "\n",
        "\n",
        "\n",
        "full_dataset.head()                # wyświetl próbkę danych\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5jj77tqVvcn"
      },
      "source": [
        "## Transformacja danych\n",
        "Aby zastosować większość algorytmów uczenia maszynowego - dane wejściowe muszą być reprezentowane jako wektory liczb. Wykorzystajmy zatem narzędzia dostarczone przez Scikit-learn do tego celu. Użyjmy klasy **CountVectorizer()**, aby podzielić poszczególne dokumenty na słowa, a następnie stworzyć reprezentację \"bag of words\"\n",
        "Dla przypomnienia - \"bag of words\" tworzony jest w nastepujący sposób:\n",
        "<ol>\n",
        "<li>Przeglądamy wszystkie dostępne dokumenty i tworzymy listę wszystkich unikalnych słów jakie napotkaliśmy (słownik).</li>\n",
        "<li>Stworzona lista wyznacza nam wektor cech - każda pozycja w takim wektorze oznacza jedno z napotkanych słów.</li>\n",
        "<li>Każdy z dokumentów mapowany jest na wektor cech poprzez zapisanie ile razy każde ze słów dokumentu wystąpiło w nim.</li>\n",
        "</ol>\n",
        "Przykład wektoryzacji znajduje się poniżej:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "_fBnD5PiVvcp"
      },
      "outputs": [],
      "source": [
        "# PRZYKŁAD ------------------------------------------\n",
        "# np. Dla dwóch dokumentów:\n",
        "# Dokument 1: Ala ma kota i ma psa \n",
        "# Dokument 2: Kot ma Alę\n",
        "\n",
        "# Poszczególne kroki wyglądają następująco:\n",
        "# Lista unikalnych słów:                 [Ala, ma, kota, i, psa, Kot, Alę] \n",
        "# Szablon wektora cech:                  [  0,  0,    0, 0,   0,   0,   0] - wektor jest tyluelementowy, ile mamy  unikalnych słów \n",
        "# Osadzenie dokumenu 1 jako wektor cech: [  1,  2,    1, 1,   1,   0,   0] - słowo \"ma\" pojawia się w dok. 2 razy, \"kot\" i \"Alę\" - wcale\n",
        "# Analogicznie dokument 2:               [  0,  1,    0, 0,   0,   1,   1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdixlm55Vvcq",
        "outputId": "b201fe9d-6981-46d3-c5cb-dd45727d9a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rozmiar stworzonej macierzy: (1624, 37325)\n",
            "Liczba dokumentów: 1624\n",
            "Rozmiar wektora bag-of-words 37325\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_counts = vectorizer.fit_transform(train['text']) # stwórz macierz liczbową z danych. W wierszach mamy kolejne dokumenty, w kolumnach kolejne pola wektora cech odpowiadające unikalnym słowom (bag of words)\n",
        "X_test_counts = vectorizer.transform(test['text'])       # analogicznie jak wyżej - dla zbioru testowego.\n",
        "\n",
        "print(\"Rozmiar stworzonej macierzy: {x}\".format(x=X_train_counts.shape)) # wyświetl rozmiar macierzy. Pierwsze pole - liczba dokumentów, drugie - liczba cech (stała dla wszystkich dokumentów)\n",
        "print(\"Liczba dokumentów: {x}\".format(x=X_train_counts.shape[0]))\n",
        "print(\"Rozmiar wektora bag-of-words {x}\".format(x=X_train_counts.shape[1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1Y3G10Vvcr"
      },
      "source": [
        "### Uwaga:\n",
        "Na zbiorze treningowym użyto funkcji - fit_transform(), na testowym - transform(). <br/>\n",
        "**Dlaczego?** fit_transform() wykonuje dwie operacje - tworzy i zapisuje listę wszystkich unikalnych słów (słownik) oraz zamienia dokument na wektor o długości takiej jak słownik. transform() natomiast wykorzystuje istniejący już słownik i wykonuje z jego użyciem transformację do wektora. \n",
        "<br/>\n",
        "Ponieważ zbiór treningowy jest zazwyczaj liczniejszy - z reguły znajdziemy w nim więcej różnych słów. Ponadto, wszystkie słowa, które mogą pomóc w klasyfikacji i tak muszą znaleźć się w zbiorze treningowym aby móc się nauczyć ich wykorzystania. Nie nadpisuje się zatem słownika za pomocą zbioru testowego i tworzy się go tylko raz - podczas treningu, wykorzystując go następnie do tworzenia nowych wektorów z nieobserwowanych podczas treningu dokumentów.\n",
        "\n",
        "# Zadanie 1 (1 punkt):\n",
        "Jak się pewnie domyślasz - reprezentacja bag-of-words będzie miała bardzo wiele zer w wygenerowanych macierzach (macierzach, w których w poszczególnych wierszach będziemy mieli poszczególne dokumenty, a w kolumnach wektory słów reprezentacji bag of words). Rozmiar macierzy z poprzedniego listingu pokazuje, że każdy dokument opisany jest wektorem 37325 pozycji, ponieważ tyle różnych słów zostało wykrytych po analizie wszystkich dokumentów treningowych. Większość dokumentów analizowanych osobbno zawierać będzie pewnie co najwyżej kilkadziesiąt/kilkaset różnych słów.\n",
        "\n",
        "***Zadanie: Napisz fragment kodu, który zliczy:***\n",
        "<ol>\n",
        "    <li><strong>jaki procent macierzy X_train_counts ma elementy o wartości różnej od zera</strong></li>\n",
        "    <li><strong>ile tokenów (łącznie, nie tylko unikalne) występuje w macierzy X_train_counts?</strong></li>\n",
        "</ol>\n",
        "Wskazówka - ponieważ zer w tej macierzy jest istotnie dużo - dane po transformacji CountVectorizerem trzymane są w specjalnym formacie, w którym zapisuje się tylko elementy mające wartości różne od zera, w tzw. macierzy rzadkiej (sparse matrix). Aby przeiterować po takiej macierzy, można wykorzystać następujące fragmenty kodu:\n",
        "\n",
        "<strong>cx = X_train_counts.tocoo()</strong> - transformuj macierz do reprezentacji koordynatowej (patrz niżej) <br/>\n",
        "<strong>for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):</strong> - pozwala ona na iterowanie po wszystkich niezerowych elementach, w każdym kroku otrzymując 3 zmienne - numer wiersza (numer dokumentu), numer kolumny (identyfikator słowa ze słownika) oraz licznik mówiący ile razy dane słowo wystąpiło w danym dokumencie. <span style=\"color: #ff0000\">(Do wykonania zadania musisz zaktualizować linijki 3, 7 i 8)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waHcWiCaVvcs",
        "outputId": "6b6cbfc4-4aed-44b0-e9ae-af0f0b7c6370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W datasecie znajduje się: 435292 tokenów. Macierz posiada 0.394% elementów niezerowych\n"
          ]
        }
      ],
      "source": [
        "count_tokens = 0   # tu zapisz liczbę wszystkich tokenów w macierzy\n",
        "count_nonzero = 0  # tu zapisz ilość elementów niezerowych w macierzy\n",
        "count_all = X_train_counts.shape[0] * X_train_counts.shape[1]      # tu zapisz ilość komórek w macierzy (ilość wierszy * ilość kolumn, rozważ użycie pola 'shape' na macierzy X_train_counts)\n",
        "\n",
        "cx = X_train_counts.tocoo()\n",
        "\n",
        "for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):    #iteracja po elementach niezerowych\n",
        "    count_tokens += count\n",
        "    count_nonzero += 1  \n",
        "\n",
        "print(\"W datasecie znajduje się: {tokens} tokenów. Macierz posiada {nonzero_percent}% elementów niezerowych\".format(\n",
        "    tokens=count_tokens,\n",
        "    nonzero_percent = round(100.0*count_nonzero/count_all, 3)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyO8zA1-Vvcs"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    <strong>Oczekiwany rezultat:</strong> <br/>\n",
        "Mniej niż 1% elementów niezerowych (!) <br/>\n",
        "Ponad 400000 tokenów\n",
        "</div>\n",
        "\n",
        "# Zadanie 2 (1 punkt) - słowa charakteryzujące klasy\n",
        "\n",
        "Wykorzystajmy macierz X_train_counts wykorzystywaną w poprzednim zadaniu, a także etykiety kategorii, aby stworzyć listy najczęściej występuących słów w danych kategoriach. <br/><br/>\n",
        "Aby ułatwić zadanie, utworzono większość funkcji **get_top_occuring_words()** tworzącej taki ranking<br/>\n",
        "**Zadanie 2a (0.5 punktu)**: twoim zadaniem jest zaktualizowanie wartości pola: **category_word_counts[category][word]**, tak, aby poprawnie zliczyć ile razy dane słowo wystąpiło w kategorii. <span style=\"color: #ff0000\">(zaktualizuj linijkę 20)</span>\n",
        "<br/>\n",
        "Czy najczęstsze słowa pozwalają rozdzielić kategorie SPAM od HAM?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ut5LqUuVvct",
        "outputId": "156e4668-13f8-462f-d47c-537ed1290b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham: ['the', 'to', 'of', 'and', 'is', 'in', 'it', 'that', 'for', 'you', 'on', 'with']\n",
            "spam: ['the', 'of', 'to', 'and', 'in', 'you', 'nbsp', 'for', 'is', 'your', 'this', 'as']\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "\n",
        "def get_top_occuring_words(X_train_counts, how_many_words, vectorizer, train):\n",
        "    id_to_word = {v: k for k, v in vectorizer.vocabulary_.items()} # stwórz mapowanie pozycji wektora bag-of-words na konkretne słowa\n",
        "    cx = X_train_counts.tocoo()\n",
        "    \n",
        "    category_word_counts = dict()      # słownik, w którym przeprowadzimy zliczanie\n",
        "    \n",
        "    for doc_id, word_id, count in zip(cx.row, cx.col, cx.data):\n",
        "        category = train.iloc[doc_id]['label']  # w category znajduje się idetyfikator kategorii dla aktualnego dokumentu, zapisujemy go\n",
        "        word = id_to_word[word_id]              # w word - aktualne słowo z dokumentu\n",
        "                                                # mamy też liczność wystąpienia danego słowa w dokumencie (gdzie? :) )\n",
        "            \n",
        "        if category not in category_word_counts.keys(): # stwórzmy słownik z kategoriami jako kluczami\n",
        "            category_word_counts[category] = dict()     # jeśli widzimy nową kategorię - dodajemy do słownika\n",
        "\n",
        "        if word not in category_word_counts[category]: # w ramach każdej kategorii będziemy zliaczać słowa\n",
        "            category_word_counts[category][word] = 0.0 # jeśli aktualne słowo jeszce nie zotało uwzględnione w kategorii - zainicjujmy jego licznik liczbą 0\n",
        "\n",
        "        category_word_counts[category][word] += count\n",
        "        \n",
        "    for category_name in category_word_counts.keys(): # wyświetl nazwy kategorii i n najczęściej występujących w nich słów\n",
        "        sorted_cat = sorted(category_word_counts[category_name].items(), key=operator.itemgetter(1), reverse=True) # posortowany dict() słowo -> liczność, wg liczności, malejąco\n",
        "        print(\"{cat}: {top}\".format(cat=category_name, top=[word for word, count in sorted_cat[:how_many_words]])) # wyświetl nazwę kategorii i top n słów\n",
        "\n",
        "\n",
        "get_top_occuring_words(X_train_counts, 12, vectorizer, train) # wywołanie funkcji\n",
        "\n",
        "#Najczęstsze słowa nie pozwalają rozdzielić kategorii SPAM od HAM, gdyż w obydwu przypadkach wyrazy się powtarzają i pasują do każdej kategorii (mają małą użyteczność)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIx2RqKHVvcv"
      },
      "source": [
        "### Wektoryzacja Tf-Idf\n",
        "\n",
        "Po wykonaniu zadania 2a widzimy, że najczęściej występujące słowa w każdej kategorii mają niewielką użytezczność (pasują do każdej kategorii). Aby sprawić, żeby na czele rankingu znalazły się słowa charakterystyczne dla danej klasy, możemy użyć metody Tf-idf. <br/>\n",
        "**Zadanie 2b:\n",
        "Nadpisz wartości X_train_counts oraz X_test_counts wykorzystując w tym celu TfidfVectorizer** (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) zamiast CountVectorizer, ustaw parametr max_df na 0.4 (tzn. ignoruj słowa, które występują w więcej niż 40% dokumentów). Następnie wykonaj stworzoną w zadaniu 2 funkcję get_top_occuring_words(), aby sprawdzić, czy ranking najważniejszych słów się zmienił. Czy zmieniony zestaw słów lepiej reprezentuje kategorie? <span style=\"color: #ff0000\">(zaktualizuj linie 3, 4, 5)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Epgp-94NVvcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3c1163-c3a6-4ff3-f1a1-300d2794d432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham: ['debian', 'org', 'lists', '20', 'unsubscribe', 'linux', 'net', 'wrote', 'list', 'my', 'can', 'www']\n",
            "spam: ['nbsp', 'our', '20', 'click', 'here', 'spam', '2009', 'content', 'hibody', 'we', 'free', 'all']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df = 0.4)\n",
        "X_train_counts = vectorizer.fit_transform(train['text']) # stwórz macierz wektorów. W wierszach mamy kolejne dokumenty, w kolumnach kolejne pola wektora cech odpowiadające unikalnym słowom\n",
        "X_test_counts = vectorizer.transform(test['text']) # analogicznie dla zbioru testowego.\n",
        "\n",
        "get_top_occuring_words(X_train_counts, 12, vectorizer, train) # wywołanie funkcji\n",
        "\n",
        "#Zmieniony zestaw słów zdecydowanie lepiej reprezentuje kategorie. W zbiorze słów dla SPAM znajdują się już bardziej typowe dla tej kategorii wyrazy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptmBJ2waVvcw"
      },
      "source": [
        "# Zadanie 3 - Stemming i lematyzacja (1 punkt)\n",
        "Często istotne słowa występują w wielu odmianach (szczególnie w językach fleksyjnych, takich jak nasz), np: university - universities ; pay - paid - paying - pays . Wielość odmian słów ma swoje przełożenie na rozmiar słownika.\n",
        "<br/><br/>\n",
        "W niektórych warunkach, w szczególności:\n",
        "<ul>\n",
        "<li>Kiedy mamy ograniczoną pamięć</li>\n",
        "<li>Kiedy ważny jest dla nas czas działania algorytmu</li>\n",
        "<li>Kiedy istnieje ryzyko przeuczenia</li>\n",
        "</ul>\n",
        "warto rozważyć znormalizowanie słów, tak, aby zmniejszyć rozmiar słownika, a co za tym idzie wymagania pamięciowe (a co za tym idzie - czas treningu/klasyfikacji). Ograniczenie rozmiaru słownika może też zapobiec przeuczeniu. Normalizację możemy wykonać np. poprzez zastosowanie stemmingu lub lematyzacji dla poszczególnych wyrazów.\n",
        "<br/>\n",
        "<strong>Zadanie 3a (0.5 punktu)</strong>: Z użyciem biblioteki NLTK wykonaj zarówno lematyzację (używając WordNetLemmatizer) jak i stemming (używając PorterStemmer) tekstu zawartego w sample_text. Uwaga - lematyzator opcjonalnie wymaga pos-tagu dla tokenu. Przekaż do funkcji lematyzującej zmienną current_word_postag jako drugi argument. <span style=\"color: #ff0000\">(zaktualizuj linie 20, 21, 34, 35)</span>\n",
        "\n",
        "<strong>Zadanie 3b (0.5 punktu)</strong>: O ile zmniejszyła się liczba unikalnych słów po zastosowaniu lematyzacji? Odpowiedź zawrzyj w komentarzu. <span style=\"color: #ff0000\">(linijki 43:45)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "z2A9a1a_Vvcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2f4e8d-e467-4987-b7c7-1f5f7dd6947c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Bazowy tekst:        There are some cheaper alternatives for buying the red trousers. There is a discount, it is so cheap!\n",
            "Wystemowany tekst:   there are some cheaper altern for buy the red trouser . there is a discount , it is so cheap !\n",
            "Zlematyzowany tekst: There be some cheap alternative for buy the red trouser . There be a discount , it be so cheap !\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):     # lematyzator wymaga, aby dla danego słowa podać mu, czy jest to czasownik, rzeczownik czy inny POS-tag. Funkcja jest adapterem tagów nadanych przez funkcję pos_tag do tagów wymaganych przez lematyzator \n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        # As default pos in lemmatization is Noun\n",
        "        return wordnet.NOUN\n",
        "    \n",
        "wordnet_lemmatizer = WordNetLemmatizer() \n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "sample_text = \"There are some cheaper alternatives for buying the red trousers. There is a discount, it is so cheap!\"\n",
        "\n",
        "lemmatized = [] # tutaj będziemy dopisywać zlematyzowane słowa\n",
        "stemmed = []    # tutaj będziemy dopisywać wystemowane słowa\n",
        "\n",
        "tokenized = word_tokenize(sample_text)     # dzielimy tekst na słowa\n",
        "pos_tokens = nltk.pos_tag(tokenized)       # nadajemy pos-tagi (rzeczownik, czasownik przymiotnik...) każdemu słowu\n",
        "\n",
        "for i in range(len(tokenized)): # dla każdego słowa\n",
        "    current_word_postag = get_wordnet_pos(pos_tokens[i][1]) # pobieramy pos-tag słowa\n",
        "    lemmatized_token = wordnet_lemmatizer.lemmatize(tokenized[i], current_word_postag)\n",
        "    stemmed_token = porter_stemmer.stem(tokenized[i])\n",
        "    \n",
        "    lemmatized.append(lemmatized_token)\n",
        "    stemmed.append(stemmed_token)\n",
        "print(\"Bazowy tekst:        {t}\".format(t=sample_text))\n",
        "print(\"Wystemowany tekst:   {t}\".format(t=\" \".join(stemmed)))\n",
        "print(\"Zlematyzowany tekst: {t}\".format(t=\" \".join(lemmatized)))\n",
        "\n",
        "# Ile uniklanych tokenów znajduje się w tekście bazowym?: 19\n",
        "# Ile unikalnych tokenów znajduje się w tekście wystemowanym?: 19\n",
        "# Ile unikalnych tokenów w tekście zlematyzowanym?: 17\n",
        "# Różnica ilości unikalnych tokenów między tekstem bazowym a wystemowanym i bazowym a zlematyzowanym: (Bazowy - Wystemowany): 0 || (Bazowy - Zlematyzowany): 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCUs6vPoVvcy"
      },
      "source": [
        "---\n",
        "# Zadanie 4 (1 punkt) - klasyfikacja i interpretacja wyników\n",
        "Mając już dobrą reprezentację danych i wiedząc jak działa normalizacja - możemy klasyfikować! <br/>\n",
        "Istnieje wiele algorytmów, które dobrze radzą sobie z klasyfikacją tekstu, kilka przykładów to: \n",
        "<ul>\n",
        "<li>Naiwny klasyfikator Bayesa</li>\n",
        "<li>Maszyna wektorów nośnych - SVM</li>\n",
        "<li>Sieci neuronowe</li>\n",
        "</ul>\n",
        "O sieciach neuronowych więcej powiemy na jednych z przyszłych laboratoriów. <br/>\n",
        "<strong>Zadanie 4a (0.5 punktu)</strong> Wykorzystując przetworzoną postać danych: X_train_counts, X_test_counts z poprzednich zadań oraz dokumentację sklearn, zaimplementuj klasyfikację z użyciem naiwnego klasyfikatora Bayesa (MultinomialNB). <span style=\"color: #ff0000\">(zaktualizuj linie 7, 9, 12)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "queOlcPLVvcz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626ce475-1bbc-4a17-8648-970f3ad39e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ile elementów testowych udało się poprawnie zaklasyfikować?\n",
            "0.8840381991814461\n",
            "Szczegółowy raport (per klasa)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.86      1.00      0.92       517\n",
            "        spam       1.00      0.61      0.76       216\n",
            "\n",
            "    accuracy                           0.88       733\n",
            "   macro avg       0.93      0.80      0.84       733\n",
            "weighted avg       0.90      0.88      0.87       733\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def labels_as_strings(vector_of_indices): # funkcja pomocnicza zamieniająca identyfikatory numeryczne na tekstowe\n",
        "    return ['ham' if ind == 0 else 'spam' for ind in vector_of_indices] \n",
        "\n",
        "nb = MultinomialNB() # STWÓRZ KLASYFIKATOR\n",
        "\n",
        "nb.fit(X_train_counts, train['label_num']) # WYTRENUJ KLASYFIKATOR\n",
        "\n",
        "print(\"Ile elementów testowych udało się poprawnie zaklasyfikować?\")\n",
        "accuracy = nb.score(X_test_counts, test['label_num']) # OBLICZ TRAFNOŚĆ\n",
        "print(accuracy)\n",
        "print(\"Szczegółowy raport (per klasa)\")\n",
        "print(classification_report(labels_as_strings(test['label_num']), labels_as_strings(nb.predict(X_test_counts)))) # testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWSmXRGqVvc0"
      },
      "source": [
        "**Zadanie 4b (0.5 punktu)\n",
        "Po analizie szczegółowego raportu z zadania 4a - odpowiedz na poniższe pytania i zapisz odpowiedzi w komentarzu:**\n",
        "<ol>\n",
        "<li>Która miara mówi nam o tym, jak wiele spośród elementów uznanych za spam rzeczywiście jest spamem?</li>\n",
        "<li>Która miara mówi nam o tym, jak wiele spośród wszystkich elementów rzeczywiście będących spamem zostało wykrytych jako spam?</li>\n",
        "<li>Która kategoria została w ogólnym rozrachunku lepiej rozpoznana przez klasyfikator, jeśli zależy nam bardziej na tym, żeby klasyfikator, jeśli mówi, że coś należy do danej klasy, raczej się w tym nie mylił, niż żeby wykrył wszystkie elementy klasy?</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "zxfeGh7wVvc0"
      },
      "outputs": [],
      "source": [
        "# odp zad 4.1: precision\n",
        "# odp zad 4.2: recall\n",
        "# odp zad 4.3: spam (większe precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEWRy1TRVvc1"
      },
      "source": [
        "Sklearn jest bardzo wdzięcznym narzędziem, w którym proces klasyfikacji możemy wykonać w zaledwie kilku linijkach. Bardzo przydatną klasą jest klasa Pipeline, która definiuje sekwencję kroków, które wykonujemy wywołując metodę fit().\n",
        "W naszym przypadku mamy dwa kroki:\n",
        "<ol>\n",
        "    <li>Wektoryzacja - zamienia dane zapisane w postaci tekstowej na macierz z wektorami bag-of-words.</li>\n",
        "    <li>Klasyfikacja - wytrenowanie klasyfikatora.</li>\n",
        "</ol>\n",
        "W zdefiniowanym obiekcie typu pipeline, i+1 element pipeline'u na wejściu dostaje dane z wyjścia i-tego elementu (Zatem nasz klasyfikator otrzyma dane przetworzone przez TfidfVectorizer). <br/>\n",
        "Metoda fit na wejściu przyjmuje listę dokumentów w formie tekstowej, oraz oczekiwane etykiety w formie liczbowej.\n",
        "<br/>\n",
        "Analogicznie w procesie klasyfikowania nowych tekstów z użyciem istniejącego modelu - metoda predict() wykona sekwencję kroków: wektoryzacja + klasyfikacja dla zadanej listy surowych tekstów). <br/>\n",
        "Zapoznaj się z poniższym kodem i uruchom go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "NWLuMZoiVvc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2203b754-7571-4750-cfe4-6efae7c0b190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekst NEED TO FIND SOMETHING? ::FREE MORTGAGE QUOTE:: To be removed from this list, click here. , zaklasyfikowany został jako: SPAM\n",
            "W zbiorze testowym 88.4038199181446% przypadków zostało poprawnie zaklasyfikowanych!\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas\n",
        "import numpy as np\n",
        "\n",
        "# ------------------- WCZYTANIE DANYCH -----------\n",
        "\n",
        "import s3fs\n",
        "full_dataset = pandas.read_csv(\"https://dwisniewski-put-pjn.s3.eu-north-1.amazonaws.com/spam_emails.csv\")      # wczytaj dane z pliku CSV\n",
        "full_dataset['label_num'] = full_dataset.label.map({'ham':0, 'spam':1})  # ponieważ nazwy kategorii zapisane są z użyciem stringów: \"ham\"/\"spam\", wykonujemy mapowanie tych wartości na liczby, aby móc wykonać klasyfikację. \n",
        "\n",
        "np.random.seed(0)                                       # ustaw seed na 0, aby zapewnić powtarzalność eksperymentu\n",
        "train_indices = np.random.rand(len(full_dataset)) < 0.7 # wylosuj 70% wierszy, które znajdą się w zbiorze treningowym\n",
        "\n",
        "train = full_dataset[train_indices] # wybierz zbior treningowy (70%)\n",
        "test = full_dataset[~train_indices] # wybierz zbiór testowy (dopełnienie treningowego - 30%)\n",
        "\n",
        "\n",
        "# ------------------- STWORZENIE PIPELINE'U -----------\n",
        "    \n",
        "pipeline = Pipeline([             # stwórzmy pipeline surowy tekst -> TFIDF vectorizer -> klasyfikator \n",
        "    ('tfidf', TfidfVectorizer(max_df=0.4)),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "# ------------------- TRANSFORMACJA I UCZENIE -----------\n",
        "\n",
        "pipeline.fit(train['text'], train['label_num']) # zwektoryzujmy dane i wytrenujmy klasyfikator na zbiorze treningowym\n",
        "\n",
        "# ------------------- KLASYFIKACJA PRZYKŁADOWEGO TEKSTU -----------\n",
        "\n",
        "text_to_predict = \"NEED TO FIND SOMETHING? ::FREE MORTGAGE QUOTE:: To be removed from this list, click here. \"\n",
        "predicted = pipeline.predict([text_to_predict])\n",
        "if predicted == 1:\n",
        "    detected = 'SPAM'\n",
        "else:\n",
        "    detected = 'HAM'\n",
        "print(\"Tekst {t}, zaklasyfikowany został jako: {d}\".format(t=text_to_predict, d=detected))\n",
        "\n",
        "# ------------------- OCENA KLASYFIKATORA -----------\n",
        "accuracy = pipeline.score(test['text'], test['label_num'])\n",
        "print(\"W zbiorze testowym {n}% przypadków zostało poprawnie zaklasyfikowanych!\".format(\n",
        "    n=100.*accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "silhUFdgVvc3"
      },
      "source": [
        "# Zadanie 5 (1 punkt): dobór parametrów klasyfikacji\n",
        "Poniżej znajduje się kod tworzący pipeline składający się z dwóch elementów: TfidfVectorizera oraz klasyfikatora naiwnego Bayesa - MultinomialNB. Wektoryzator tworzy model bag-of-words, który uwzględnia jedynie 1000 najważniejszych słów w słowniku. W celu zastosowania stemmingu oraz lematyzatora w treningu i predykcji stoworzona została klasa TheTokenizer, która poza podziałem tekstu na słowa wykonuje również zadania normalizacji wg. ustalonych flag: **use_stemming, use_lemmatization, use_stopword_removal**. <br/>\n",
        "<strong>Zadanie 5a (0.5 punktu)</strong>: <br/>\n",
        "Zweryfikuj jak zmiana wartości flag **use_stemming, use_lemmatization, use_stopword_removal**, a co za tym idzie wykorzystanie lamatyzacji, stemmingu i usuwania najczęstszych słow wpływa na miary precision, recall i f1 stworzonego klasyfikatora. Wyniki zapisz w komentarzu. <span style=\"color: #ff0000\">(modyfikuj linie 16, 17, 18, komentarz - w kolejnej komórce)</span><br/>\n",
        "<strong> Zadanie 5b (0.5 punktu)</strong>: <br/>\n",
        "Ustaw flagi **use_stemming, use_lemmatization, use_stopword_removal** z linii 16,17 i 18 na False, i porównaj wartości precision recall i f1 dla klasyfikatora, ktory wykorzystuje CountVectorizer i takiego, który wykorzystuje TfidfVectorizer. Pozostaw parametr max_features=1000 w obu przypadkach. Który wektoryzator jest lepszy? <span style=\"color: #ff0000\">(modyfikuj linię 84)</span>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "j0hjgcdvVvc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6116ec2-8681-4795-f8a0-a6d4c383fd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Pobieranie danych\n",
            "Tworzenie pipeline'u\n",
            "Uczenie pipeline'u\n",
            "Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika?)?\n",
            "W słowniku znajduje się 1000 różnych słów\n",
            "Ocena klasyfikatora\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "         comp.graphics       0.69      0.67      0.68       389\n",
            " comp.sys.mac.hardware       0.60      0.73      0.66       385\n",
            "          misc.forsale       0.77      0.88      0.82       390\n",
            "       rec.motorcycles       0.59      0.83      0.69       398\n",
            "       sci.electronics       0.64      0.57      0.60       393\n",
            "               sci.med       0.81      0.52      0.64       396\n",
            "             sci.space       0.81      0.67      0.73       394\n",
            "soc.religion.christian       0.87      0.84      0.85       398\n",
            "    talk.politics.guns       0.57      0.54      0.55       364\n",
            "    talk.politics.misc       0.51      0.51      0.51       310\n",
            "\n",
            "              accuracy                           0.68      3817\n",
            "             macro avg       0.69      0.67      0.67      3817\n",
            "          weighted avg       0.69      0.68      0.68      3817\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# wczytywanie danych\n",
        "from sklearn.datasets import fetch_20newsgroups # zbiór danych zawarty w Sklearn, który zawiera dane z 20 grup newsowych\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk import word_tokenize, pos_tag       \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "class TheTokenizer(object):              # Aby zastosować lematyzację/stemming z NLTK, musimy napisać własny tokenizator, który podzieli tekst na słowa i przekształci je na stemy/lematy. \n",
        "    def __init__(self):\n",
        "        self.use_stemming = False         #czy stemować?\n",
        "        self.use_lemmatization = False     #czy lematyzować?\n",
        "        self.use_stopword_removal = False  #czy usunąć słowa częste jak the, and, of itp.\n",
        "        \n",
        "        self.wnl = WordNetLemmatizer()   # Utwórz lematyzator oparty na wordnet\n",
        "        self.stemmer = PorterStemmer()   # Utwórz stemmer Portera\n",
        "        self.stopwords = set(stopwords.words('english')) # załaduj listę ~100 najczęstszych słów (the, and, of, ...)\n",
        "    \n",
        "    def __call__(self, doc):\n",
        "        if not self.use_stemming and not self.use_lemmatization: # tokenizuj i ew. lematyzuj/stemuj/usuń stopwords w zależności od ustawionych flag\n",
        "            return [t for t in word_tokenize(doc) if self.allow(t)]\n",
        "        elif self.use_stemming and not self.use_lemmatization:\n",
        "            return [self.stem_token(t) for t in word_tokenize(doc) if self.allow(t)]\n",
        "        elif self.use_lemmatization and not self.use_stemming:\n",
        "            return [self.lemmatize_token(t, pos) for t, pos in pos_tag(word_tokenize(doc)) if self.allow(t)]\n",
        "    \n",
        "    def stem_token(self, t):\n",
        "        return self.stemmer.stem(t)\n",
        "    \n",
        "    def lemmatize_token(self, t, postag):\n",
        "        return self.wnl.lemmatize(t, self.get_wordnet_pos(postag))\n",
        "    \n",
        "    def allow(self, t):\n",
        "        if not self.use_stopword_removal:\n",
        "            return True\n",
        "        \n",
        "        if t in self.stopwords:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "        \n",
        "    def get_wordnet_pos(self, treebank_tag):\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            # As default pos in lemmatization is Noun\n",
        "            return wordnet.NOUN\n",
        "\n",
        "def labels_as_strings(vector_of_indices): # funkcja pomocnicza zamieniająca identyfikatory numeryczne na tekstowe\n",
        "    return [dataset_train.target_names[ind] for ind in vector_of_indices] \n",
        "\n",
        "\n",
        "print(\"Pobieranie danych\")\n",
        "categories = ['misc.forsale', 'soc.religion.christian', 'sci.space', 'talk.politics.guns',\n",
        "              'comp.graphics', 'sci.med',  'rec.motorcycles',  'sci.med',\n",
        "              'sci.electronics', 'talk.politics.misc', 'comp.sys.mac.hardware'] # lista kategorii, które chcemy analizować\n",
        "\n",
        "dataset_train = fetch_20newsgroups(subset='train',\n",
        "                                   categories=categories,\n",
        "                                   shuffle=True,\n",
        "                                   random_state=42) # pobieramy zbiór uczący (na nim będziemy trenować) dla wybranych kategorii.\n",
        "    \n",
        "\n",
        "dataset_test = fetch_20newsgroups(subset='test',\n",
        "                                  categories=categories,\n",
        "                                  shuffle=True,\n",
        "                                  random_state=42) # pobieramy zbiór testowy (na nim będziemy testować) dla wybranych kategorii\n",
        "\n",
        "vectorizer_tfid = TfidfVectorizer(tokenizer=TheTokenizer(), max_features=1000)\n",
        "vectorizer_count = CountVectorizer(tokenizer=TheTokenizer(), max_features=1000)\n",
        "\n",
        "print(\"Tworzenie pipeline'u\")\n",
        "pipeline = Pipeline([             # stwórzmy pipeline surowy tekst -> vectorizer -> klasyfikator \n",
        "    ('vectorizer', vectorizer_count),\n",
        "    ('clf', MultinomialNB()),\n",
        "])\n",
        "\n",
        "\n",
        "print(\"Uczenie pipeline'u\")\n",
        "pipeline.fit(dataset_train.data, dataset_train.target) # trenujemy klasyfikator!\n",
        "\n",
        "\n",
        "print(\"Ile różnych słów tworzy wektor bag of words (jaki jest rozmiar słownika?)?\")\n",
        "print(\"W słowniku znajduje się {n} różnych słów\".format(\n",
        "    n=len(pipeline.named_steps['vectorizer'].vocabulary_.keys())\n",
        "))\n",
        "\n",
        "\n",
        "print(\"Ocena klasyfikatora\")\n",
        "print(classification_report(labels_as_strings(dataset_test.target), labels_as_strings(pipeline.predict(dataset_test.data)))) # testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s0w7rj2_Vvc5"
      },
      "outputs": [],
      "source": [
        "Top 1000 - CountVectorizer\n",
        "        Precision: 0.69  Recall: 0.67   F1: 0.67 \n",
        "\n",
        "Top 1000 - TfIDF:\n",
        "    Bez usuwania stopwords:\n",
        "        Precision: 0.74   Recall: 0.72   F1: 0.72\n",
        "    Usuwanie stopwords: \n",
        "        Precision: 0.76   Recall: 0.74   F1: 0.74\n",
        "    Usuwanie stopwords + stemming:   \n",
        "        Precision: 0.79   Recall: 0.77   F1: 0.77\n",
        "    Usuwanie stopwords + lematyzacja:\n",
        "        Precision: 0.79   Recall: 0.78   F1: 0.78\n",
        "    Lematyzacja:\n",
        "        Precision: 0.77   Recall: 0.76   F1: 0.75\n",
        "    Stemming:\n",
        "        Precision: 0.78   Recall: 0.76   F1: 0.76\n",
        "\n",
        "#Lepsze wyniki osiągamy przy zastosowaniu TfidVectorizer"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Zadania_lab2_Kowalewski_Bartlomiej_Kowalewski_L15_145204.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}